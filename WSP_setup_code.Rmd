---
title: "WSP setup and demographics code"
author: "Lizzie Jones^[University of Brighton, l.jones4@brighton.ac.uk]"
date: "02/05/2021"
output:
  html_document:
    number_sections: yes
  pdf_document:
    number_sections: yes
  word_document: default
---

WSP - Initial data exploration
====================

#### About R Markdowns

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>. To generate the document of all content, click the **Knit** button. To change the output (e.g. PDF, HTML) change the 'output' at the top to any of the outputs listerd here: https://rmarkdown.rstudio.com/lesson-9.html.


```{r setup/packages, include=FALSE, warning=FALSE}

# This code assumes that all packages are installed. If not, use 'install.packages' e.g. install.packages("likert")
# install.packages("hms")

# Key packages for data wrangling
wrangling <- c("dplyr","tidyverse","purr","magrittr","lubridate","hms",
             "data.table","plyr","tidyr","tibble","reshape2")
lapply(wrangling, require, character.only = TRUE) 

# Useful survey analysis packages
survey <- c("likert","careless")
lapply(survey, require, character.only = TRUE) 

# Useful packages for statistics
stats <- c("stats","ggpubr","lme4","MASS","car","psych",
                   "MuMIn","glmmTMB","nlme","DHARMa")
lapply(stats, require, character.only = TRUE) 

# Useful packages for text analysis
# text <- c("tm","tau","koRpus","lexicon","sylly","textir",
#          "textmineR","MediaNews", "lsa","SemNeT","ngram","ngramrr",
#          "corpustools","udpipe","textstem", "tidytext","text2vec")
# lapply(text, require, character.only = TRUE) 

# Favourite data visualisation packages
vismap <- c("ggvis","htmlwidgets","maps", "lattice","ggmap","ggplot2","plotly","rnaturalearth",
            "RColorBrewer", "sjPlot", "ggrepel", "rgdal", "maptools", "gpclib")
lapply(vismap, require, character.only = TRUE) 
gpclibPermit()  # Gives maptool permission to use gpclib


```

```{r working directories and data upload, include=FALSE}

# Load in working  directory and datasets
setwd("~/Documents/White-Stork-Project")

# Load in the data
original_data <- read.csv("Stork_MainDataset.csv", header = TRUE, stringsAsFactors=TRUE)
all_data <- read.csv("Stork_Dataset_Radapted.csv", header = TRUE, stringsAsFactors=TRUE)

```


```{r column grouping, check formats, view data, include=FALSE}
## Here I create a way to call in groups of columns (e.g. Q17.1 - 17.13) as one unit to enable easier and more generalisable functions, data comparison and plotting.

# Original column names
glimpse(original_data)
# R adapted column names
colnames(all_data)

# Grouping columns by selecting Question numbers (and dropping scores or open questions)
overall_score_colnames <- select(all_data, ends_with("overallscore"))
Q5_diet_colnames <- select(all_data, starts_with("Q5"))
Q6_habitat_colnames <- select(all_data, starts_with("Q6"))
Q10_cursource_colnames <- select(all_data, starts_with("Q10a"))
Q10_prefsource_colnames <- select(all_data, starts_with("Q10b"))
Q13_colnames <- select(all_data, starts_with("Q13"), -ends_with('score'))
Q14_colnames <- select(all_data, starts_with("Q14"), -ends_with('score'))
Q17_colnames <- select(all_data, starts_with("Q17"), -ends_with('open'))
Q18_colnames <- select(all_data, starts_with("Q18_"), -ends_with('open')) # Frequency of
Q19_colnames <- select(all_data, starts_with("Q19_"), -ends_with('score')) # Q19 total score = NCI
Q21_colnames <- select(all_data, starts_with("Q21_"), -ends_with('score')) # Q21 total score = ProCoBS
Q23_colnames <- select(all_data, starts_with("Q23_"), -ends_with('score'))

# Sample size per column 
NROW(na.omit(all_data$Q2_photo_recog_score))

```


## Data cleaning

First I have conducted some data cleaning to identify any respondents or data points that need to be removed and explain why. First I converted the 'TimeTaken' column to a total number of seconds (SecsTaken) for easier to more easily investigate means and quantiles. I initially focussed on the fastest 10% of respondents across both surveys as they are most likely to have straightlined through the survey. I visually inspected the data, then used the 'careless' package to find evidence of straightliningm 'even-odd' consistencies, and intra-individual response variability (IRV), across the whole survey and within the multiple choice questions (particularly questions 4, 5, 13, 15, 16, 17, 22, 23, 24)


```{r data formatting, warning=FALSE, messages=FALSE}

## Cleaning full dataset to prevent having to do code twice for both samples
all_data$Age_group_match <- all_data$Age_group # Create new column with matching age-group formats
all_data <- all_data  %>%
  dplyr::mutate(Age_group_match = recode(Age_group_match, "c('65-74', '75 and over')='65+'"))
summary(all_data$Age_group_match)

# Formatting date and time columns
as.Date(all_data$StartDate, format = "%d/%m/%Y")
as.Date(all_data$CompletionDate, format = "%d/%m/%Y")

```

```{r data time-checks, warning=FALSE, messages=FALSE}

### Explore average time taken to complete questionnaire and check for straightlining
all_data$SecsTaken <- as.numeric(lubridate::seconds(all_data$TimeTaken)) # Create numeric column of time taken (seconds)

quantile(all_data$SecsTaken, 0.1) # Fastest 10% of all respondents = completion in 188.9 seconds/ about 3 mins
quantile(all_data$SecsTaken, 0.05) # Fastest 5% of all respondents = completion in 117.95 seconds/ about 2 mins
quantile(all_data$SecsTaken, 0.025) # Fastest 2.5% of all respondents = completion in 70.975 seconds/ about 1.2 mins

fastest_10 <- subset(all_data, SecsTaken < 188.8) # Sample of fastest 10% of all respondents
fastest_5 <- subset(all_data, SecsTaken < 117.95) # Sample of fastest 5% of all respondents
fastest_2.5 <- subset(all_data, SecsTaken < 70.975) # Sample of fastest 2.5% of all respondents

summary(fastest_5$SurveyType) # 96.07% of respondents in fastest 5% are from the NatRep sample
summary(fastest_2.5$SurveyType) # 100% of respondents in fastest 2.5% are from the NatRep sample


```


```{r data checks - fastest 5%, warning=FALSE, messages=FALSE}

### Checking fastest 5% of respondents
# Checking the fastest 5% for straightlining across whole survey 
long_fastest_5 <- longstring(fastest_5, avg = FALSE) # Identifies the longest string of identical consecutive responses for each respondent
evenodd_fastest_5 <- evenodd(fastest_5, rep(5,10)) # Calculates the even-odd consistency score
irv_fast_5 <- irv(fastest_5) # Calculates the intra-individual response variability (IRV)
boxplot(irv_fast_5, main="Intra-individual response variability (IRV)")

# Checking the fastest 5% for straightlining within each set of mutliple choice questions
# Q5 diet
summary(all_data$Q5_overallscore_diet)
summary(fastest_5$Q5_overallscore_diet) ### Not a significant difference in Q5 diet score between all_data, fastest 5% and 2.5% samples
# Q6 habitat
summary(all_data$Q6_habitat_overallscore)
summary(fastest_5$Q6_habitat_overallscore)
# Overall knowledge score
summary(all_data$KnowledgeScore)
summary(fastest_5$KnowledgeScore)
# NCI
summary(all_data$NCI)
summary(fastest_5$NCI)
# Pro-cons behaviours
summary(all_data$ProCoBS)
summary(fastest_5$ProCoBS)
# Bird Interest Score
summary(all_data$BirdInterestScore)
summary(fastest_5$BirdInterestScore)

  
##### Data cleaning using the 'Careless' package
# Identifies the longest string of identical consecutive responses for each observation
careless_long <- longstring(all_data, avg = FALSE)
careless_avg <- longstring(all_data, avg = TRUE)
boxplot(careless_avg$longstr, main="Number of columns in Respondent longstring") #produce a boxplot of the longstring index
boxplot(careless_avg$avgstr, main="Average longstring index")

# Calculates the even-odd consistency score
careless_all <- evenodd(all_data, rep(5,10))
careless_alldiag <- evenodd(all_data, rep(5,10), diag = TRUE)

# Calculates the intra-individual response variability (IRV)
irv_total <- irv(all_data)
boxplot(irv_total, main="Intra-individual response variability (IRV)")
#calculate the irv over all items + calculate the irv for each quarter of the questionnaire
irv_split <- irv(all_data, split = TRUE, num.split = 4)
# boxplot(irv_split$irv4) #produce a boxplot of the IRV for the fourth quarter


```

```{r final dataframes, echo=FALSE, warning=FALSE, message=FALSE}

# View cleaned dataset
summary(all_data_clean)
### Create two dataframes: one for each data collection (Proactive survey and Nationally representative survey) for easier comparison
proact_data <- all_data_clean[which(all_data_clean$SurveyType == "Proactive"),]
natrep_data <- all_data_clean[which(all_data_clean$SurveyType == "NatRep"),]


```


\newpage


## Exploring Respondent demographics

The distribution of gender and education is explored and compared between samples using stacked bar plots.

```{r demographics, echo=FALSE, warning=FALSE, message=FALSE}

# Create seperate dataframe for all demographics
demo_columns <- c("SurveyType", "Age_group","Age_group_match", "Gender", "Education", "Education_other",
                                "Occupation", "Occupation_other", "Region", "Area_type", "Postcode", "ReleaseSite")
# Create seperate dataframe for All data, Nationally representative and Proactive demographics
all_demo <- all_data %>% select(demo_columns)
natrep_demo <- natrep_data %>% select(demo_columns)
proact_demo <- proact_data %>% select(demo_columns)


## Age
# Stacked barplot of gender per survey
all_age_df<- all_demo  %>%
  group_by(SurveyType, Age_group_match) %>%
  summarise(counts = n()) %>%
  mutate(Percentage = round(counts/sum(counts)*100, 2))

age_bar <- ggplot(all_age_df, aes(x = SurveyType, y = Percentage, fill = Age_group_match)) +
  geom_col(position = position_stack(reverse = TRUE)) +
  geom_text(aes(label = paste0(Percentage, "%")),
            position = position_stack(vjust = 0.5, reverse = TRUE)) +
  scale_fill_brewer(palette = "Pastel1") +
  theme_minimal(base_size = 12) + xlab("Sample")



## Gender
# Stacked barplot of gender per survey
all_gender_df<- all_demo  %>%
  group_by(SurveyType, Gender) %>%
  summarise(counts = n()) %>%
  mutate(Percentage = round(counts/sum(counts)*100, 2))

gender_bar <- ggplot(all_gender_df, aes(x = SurveyType, y = Percentage, fill = Gender)) +
  geom_col(position = position_stack(reverse = TRUE)) +
  geom_text(aes(label = paste0(Percentage, "%")),
            position = position_stack(vjust = 0.5, reverse = TRUE)) +
  scale_fill_brewer(palette = "Pastel1") +
  theme_minimal(base_size = 12) + xlab("Sample")


## Education
# Stacked barplot of gender per survey
all_education_df <- all_demo  %>%
  group_by(SurveyType, Education) %>%
  summarise(counts = n()) %>%
  mutate(Percentage = round(counts/sum(counts)*100, 2))

levels(all_education_df$Education)[levels(all_education_df$Education)=="Secondary school (GCSEs, A Levels or equivalent)"] <- "Secondary school"
levels(all_education_df$Education)[levels(all_education_df$Education)=="Postgraduate degree (Masters; Doctorate)"] <- "Postgraduate degree"

education_bar <- ggplot(all_education_df, aes(x = SurveyType, y = Percentage,
         fill = factor(Education, levels=c("Postgraduate degree",
         "Undergraduate degree","Further Education","Secondary school",
         "No formal qualifications", "Prefer not to answer", "Other")))) +
  geom_col(position = position_stack(reverse = TRUE)) +
  geom_text(aes(label = paste0(Percentage, "%")),
            position = position_stack(vjust = 0.5, reverse = TRUE)) + # geom_text_repel
  scale_fill_brewer(palette = "Pastel1") + labs(fill = "Education", x = "Sample") +
  theme_minimal(base_size = 12)



## Occupation
# Create percentages
all_occupation_df <- all_demo  %>%
  group_by(SurveyType, Occupation) %>%
  summarise(counts = n()) %>%
  mutate(Percentage = round(counts/sum(counts)*100, 2)) # Too many levels for bar plot - see table below

```

```{r demographics - stacked plots, echo=FALSE, message=FALSE, warning=FALSE}

ggarrange(age_bar, gender_bar, education_bar, ncol=1, nrow=3)

```


\newpage


### Respondent demographics table

The table below (created using the package "table1") outlines the demographic characteriscs of each of the two samples, and the overall demographics of all respondents across both samples. For each demographic variable the tables provides a breakdown of the number of respondents within each level/group and the percentage.



```{r demographics table,echo=FALSE, message=FALSE, warning=FALSE}
# Demographics table for publication
library("table1")
both_demo <- as.data.frame(all_demo)
both_demo_data <- lapply(both_demo, function(x) x[sample(c(TRUE, NA),
                                                         prob = c(0.99999, 0.00001),size = length(x), replace = TRUE)])
# Reanme variables
levels(both_demo$SurveyType)[levels(both_demo$SurveyType)=="NatRep"] <- "Nationally rep."
levels(both_demo$Education)[levels(both_demo$Education)=="Secondary school (GCSEs, A Levels or equivalent)"] <- "Secondary school"
levels(both_demo$Education)[levels(both_demo$Education)=="Postgraduate degree (Masters; Doctorate)"] <- "Postgraduate degree"
# Reorder education factor
both_demo$Education <- factor(both_demo$Education, levels=c("Postgraduate degree",
         "Undergraduate degree","Further Education","Secondary school",
         "No formal qualifications", "Prefer not to answer", "Other"))
# Format table
table1::label(both_demo$Age_group) <- "Age group"
table1::label(both_demo$Area_type) <- "Area type"
table1::label(both_demo$ReleaseSite) <- "Release site"
# Create demo table
full_demo_table <- table1::table1(~Age_group + Gender + Education + Occupation + Region + Area_type + 
                                    ReleaseSite | SurveyType, data = both_demo)

full_demo_table

```



```{r mapping postcodes, include=FALSE}

# Download UK postcode polygon Shapefile
# download.file(
#   "http://www.opendoorlogistics.com/wp-content/uploads/Data/UK-postcode-boundaries-Jan-2015.zip",
#   "postal_shapefile"
# )
unzip("postal_shapefile")

# Read the downloaded Shapefile from disk
postal <- maptools::readShapeSpatial("./Distribution/Areas")

# Assign each "region" an unique id
postal.count <- nrow(postal@data)
postal@data$id <- 1:postal.count

# Transform SpatialPolygonsDataFrame to regular data.frame in ggplot format
postal.fort <- ggplot2::fortify(postal, region='id')

# Extract first two digits of postcode and make uppercase
all_demo$postal_area_code = toupper(substr(all_data$Postcode, 1, 2))
all_demo$postal_area_code <- gsub('[0-9]+', '', all_demo$postal_area_code)

df <- all_demo %>%
  dplyr::select(postal_area_code) %>% 
  group_by(postal_area_code) %>%
  summarise(freq = n())

# Add "region" id to frequency data
df <- merge(df, postal@data, by.x="postal_area_code", by.y="name")
# Merge frequency data onto geogrphical postal polygons
postal.fort <- merge(postal.fort,  df, by="id", all.x=T, all.y=F)
postal.fort <- postal.fort[order(postal.fort$order),] # Reordering since ggplot expect data.fram in same order as "order" column

postcode_2_map <- ggplot(postal.fort) + 
  geom_polygon(aes(x = long, y = lat, group = group, fill=freq), colour="#e6f7ff") + 
  scale_fill_gradient(low = "blue", high = "red") +
  labs(fill = "Respondent frequency", x = "Longitude", y = "Latitude") +
  coord_fixed()




### Alternative mapping method using first 4 digits (finer scale)
########################################
# Read the downloaded Shapefile from disk
postcode_outcodes <- read_csv("Distribution/postcode-outcodes.csv") # linking into a folder within working directory
head(postcode_outcodes)

# Extract first two four continuous digits of postcode and make uppercase
natrep_demo$postal_area_code = gsub( "\\s.*", "", natrep_data$Postcode)
natrep_demo$postal_area_code = toupper(substr(natrep_demo$postal_area_code, 1, 4))
#Your initial list is in Df_JVT with variable PostCodes.
natrep_list <- as.list(unique(natrep_demo$postal_area_code))
#Select your postcodes from Df_UK and choose variable to display on the map
natrep_datamap <- subset(postcode_outcodes, postcode_outcodes$postcode %in% natrep_list, select= c("postcode","latitude",  "longitude"))  
row.names(natrep_datamap) <- 1:nrow(natrep_datamap)
natrep_datamap$Survey <- "Nat.Representative"

# Extract first two four continuous digits of postcode and make uppercase
proact_demo$postal_area_code = gsub( "\\s.*", "", proact_data$Postcode)
proact_demo$postal_area_code = toupper(substr(proact_demo$postal_area_code, 1, 4))
#Your initial list is in Df_JVT with variable PostCodes.
proact_list <- as.list(unique(proact_demo$postal_area_code))
#Select your postcodes from Df_UK and choose variable to display on the map
proact_datamap <- subset(postcode_outcodes, postcode_outcodes$postcode %in% proact_list, select= c("postcode","latitude",  "longitude"))  
row.names(proact_datamap) <- 1:nrow(proact_datamap)
proact_datamap$Survey <- "Proactive"

#Combine map data
datamap <- rbind(natrep_datamap, proact_datamap)

###### Mapping reserve locations following https://www.r-spatial.org/r/2018/10/25/ggplot2-sf.html 
world <- ne_countries(scale = "medium", returnclass = "sf")
class(world)
theme <- theme_set(theme())
theme_set(theme)
# Plot reserves map
postcode_4_map <- ggplot(data = world) +
  geom_sf(fill= "#EBFFDF") +
  coord_sf(xlim = c(-11, 3), ylim = c(49, 60), expand = FALSE) +
  geom_point(data = datamap, aes(x = longitude, y = latitude, colour=Survey), size = 1, shape = 19, alpha = 0.2)  +
  scale_colour_manual(values = c("blue", "red")) +
  ggspatial::annotation_scale(location = "bl", width_hint = 0.2) +
  labs(colour="Survey type", title="Postcode map") + xlab("Longitude") + ylab("Latitude") +
  theme(panel.grid.major = element_line(color = gray(.5), linetype = "dashed", size = 0.5),
        panel.background = element_rect(fill = "#e6f7ff"))
# ggsave("Ch4.reservesmap.pdf",width = 6, height = 6, dpi = 300)

```

\newpage

### Respondent postcode mapping 

Maps of respondent location using different methods: A. Map of first 1 or 2 alphabetical digits, (e.g. SW or N) for all participants with postcode boundaries, in which colour of area reflects density of participants per postcode region, and B. Map of first 4 digits of postcode (e.g., TN28), in which points are colour-coded according to survey type.


```{r postcode map, fig.cap = "Map of first 2 digits of all postcodes (e.g., SW)", fig.dim = c(10, 7), message=FALSE, echo=FALSE, warning=FALSE}
postcode_2_map
```

```{r postcode map 2, fig.cap = "Map of first 4 digits of postcode (e.g., ), colour = survey type", fig.dim = c(10, 7), message=FALSE, echo=FALSE, warning=FALSE}
postcode_4_map
```



